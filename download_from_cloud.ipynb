{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme\n",
    "\n",
    "This script contains several constituent functions which are run together in the bottom cell. Make sure to run all cells prior to running the bottom. There is an explanation above each cell as to what that function does and what inputs it takes. In summary, the whole thing is designed to: \n",
    "\n",
    "    1) log into the Aeroqual Cloud API\n",
    "    2) check for first date data are needed or missing for the monitor\n",
    "    3) request data from date identified in step 2 up to yesterday in one-week intervals\n",
    "    4) concatenate data for each monitor and write to csv\n",
    "    5) concatenate individual monitors into one master dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to aeroqual cloud. Requires inputs of username and password. This requires either Lee Ann or Boris' credentials to login to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aqc_login(username, password):\n",
    "    \n",
    "    credentials = {'UserName': username, 'Password': password}\n",
    "    \n",
    "    aqc_login = requests.post('https://cloud.aeroqual.com/api/account/login/', data = credentials)\n",
    "    \n",
    "    print('Login status:', aqc_login)\n",
    "    \n",
    "    return(aqc_login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of available instruments. This will return all instruments with data available on the cloud for any period of time and any pollutants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruments_available(aqc_login):\n",
    "    \n",
    "    available_aqys = requests.get('https://cloud.aeroqual.com/api/instrument', cookies = aqc_login.cookies)\n",
    "    \n",
    "    print('Instrument list request status:', available_aqys)\n",
    "    \n",
    "    return(available_aqys.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions below this one are intended to work on one AQY at a time. Easiest use case is to iterate through the AQY list generated in get_instruments_available. This one gets the date the desired AQY was deployed as a first theoretically possible measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deployment_date(aqy_id):\n",
    "    \n",
    "    deployment_network = pd.read_csv('monitor_deployment.txt', sep = '\\t')\n",
    "    deployment_aqy = deployment_network[deployment_network['Monitor ID'] == aqy_id]\n",
    "    \n",
    "    deployment_date = deployment_aqy.iloc[0]['Deployment date']\n",
    "    deployment_split = str.split(deployment_date, '-')\n",
    "    \n",
    "    deployment_date_dt = datetime.date(int(deployment_split[0]), int(deployment_split[1]), int(deployment_split[2]))\n",
    "    \n",
    "    return(deployment_date_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets a list of all existing days in the dataset based on the existing filepath. Feeds into _find_date_to_start_ and is used to identify gaps in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_dates_in_data(existing_file_path):\n",
    "    \n",
    "    existing_data = pd.read_csv(existing_file_path)\n",
    "    existing_data['date_observed'] = pd.to_datetime(existing_data['Time'].str[0:11])\n",
    "    \n",
    "    existing_dates = pd.DataFrame(set(existing_data['date_observed'])).rename(columns = {0: 'date_observed'}).sort_values('date_observed')\n",
    "    existing_dates['time_dif'] = existing_dates['date_observed'].diff()\n",
    "    existing_dates = existing_dates.dropna()\n",
    "        \n",
    "    return(existing_dates)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses existing days in data and set of all possible days to find first day not in observed data that should be. Returns this as first data gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO - ONLY LOOK FOR GAPS IN LAST 6-8 WEEKS TO IMPROVE RUN TIME WHILE BACKFILLING\n",
    "def find_first_data_gap(existing_dates):\n",
    "    \n",
    "    dates_observed = existing_dates['date_observed']\n",
    "    dates_observed_dt = set([datetime.date(date_observed.year, date_observed.month, date_observed.day) for date_observed in dates_observed])\n",
    "\n",
    "    first_date_observed = existing_dates['date_observed'][0]\n",
    "    yesterday = pd.Timestamp.today() - pd.Timedelta(value = 1, unit = 'D')\n",
    "    dates_possible = pd.date_range(first_date_observed, yesterday)\n",
    "    \n",
    "    dates_missing = set(dates_possible.date)\n",
    "    dates_missing.difference_update(dates_observed_dt)\n",
    "    \n",
    "    if len(dates_missing) >= 1:\n",
    "        \n",
    "        first_missing_date = min(dates_missing)\n",
    "    \n",
    "        return(first_missing_date)\n",
    "    else:\n",
    "        \n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks for existing data in results folder for given AQY. If none exist, returns deployment date as first date of data needed. This is the parent function to _get deployment date, get_existing_dates_in_data, find_first_gap_in_data, and find_start_date_from_existing_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_existing_get_start_date(aqy_id):\n",
    "\n",
    "    existing_min_data_aqy = os.path.join('raw_minute_' + aqy_id + '.csv')\n",
    "    existing_files_all_aqys = os.listdir('results_downloader_data/one_minute_data')\n",
    "    \n",
    "    deployment_date = get_deployment_date(aqy_id)\n",
    "    \n",
    "    ### USE BELOW TO FIGURE OUT SOME KIND OF CONDITIONAL EXECUTION FOR SAC DATA VS. DEPLOYMENT DATA\n",
    "    return(deployment_date - datetime.timedelta(days = 180))\n",
    "    \n",
    "    if existing_min_data_aqy in existing_files_all_aqys:  \n",
    "        \n",
    "        existing_dates = get_existing_dates_in_data(os.path.join('results_downloader_data/one_minute_data', existing_min_data_aqy))\n",
    "        first_data_gap = find_first_data_gap(existing_dates)                                         \n",
    "        \n",
    "        if first_data_gap is None:\n",
    "            \n",
    "            first_day_needed = 'None - up to date'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            first_day_needed = max(deployment_date, first_data_gap)\n",
    "        \n",
    "        return(first_day_needed)\n",
    "         \n",
    "    else:\n",
    "        \n",
    "        return(deployment_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets all necessary query dates (start of week and end of week) based on start date returned in above functions. Formats in all needed formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_query_dates(start_date):\n",
    "    \n",
    "    yesterday = datetime.date.today() - datetime.timedelta(days = 1)\n",
    "    \n",
    "    desired_dates = {'start_date_dt': start_date, \n",
    "                     'end_date_dt': None, \n",
    "                     'start_date_str': None,\n",
    "                     'end_date_str': None\n",
    "                    }\n",
    "    \n",
    "    desired_dates['end_date_dt'] = min(desired_dates['start_date_dt'] + datetime.timedelta(days = 6), yesterday)\n",
    "    \n",
    "    desired_dates['start_date_str'] = desired_dates['start_date_dt'].strftime('%Y-%m-%d')\n",
    "    desired_dates['end_date_str'] = desired_dates['end_date_dt'].strftime('%Y-%m-%d')\n",
    "    \n",
    "    return(desired_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests AQY data between the start and end date given for the requested averaging time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_request_url(aqy_id, start_date, end_date):\n",
    "    \n",
    "    request_url_base = 'https://api.cloud.aeroqual.com/V2/'\n",
    "    request_url_project = 'organisations/PSE%20Healthy%20Energy/projects/richmond_air/data?'\n",
    "    request_url_id = 'instruments/{}/data?'.format(str(aqy_id.replace(' ', '%20')))\n",
    "    request_url_dates = 'averagingPeriod=1&from={}%2000%3A00%3A00&to={}%2023%3A59%3A59&includeDiagnostics=true&rawValues=true&utc=false&includeSensorsWithNoData=true'.format(start_date, end_date)\n",
    "    \n",
    "    if aqy_id == 'Project':\n",
    "        request_url = request_url_base + request_url_project + request_url_dates\n",
    "    else:\n",
    "        request_url = request_url_base + request_url_id + request_url_dates\n",
    "    \n",
    "    return(request_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aqy_data_for_week(aqc_login, request_url): \n",
    "    \n",
    "    aqy_data = requests.get(request_url, cookies = aqc_login.cookies)\n",
    "    request_status = str(aqy_data)[-5:-2]\n",
    "    \n",
    "    if request_status != '200':\n",
    "        print('Request status:', aqy_data)\n",
    "        return(aqy_data.content)\n",
    "    \n",
    "    aqy_data_json = aqy_data.json()\n",
    "    \n",
    "    if 'Data' in aqy_data_json['Instruments'][0].keys():\n",
    "        aqy_data_df = pd.json_normalize(aqy_data_json['Instruments'][0]['Data'])\n",
    "    \n",
    "        return(aqy_data_df)\n",
    "    \n",
    "    else:\n",
    "        return(pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_results(aqy_id):\n",
    "    \n",
    "    existing_results_aqy = 'raw_minute_' + aqy_id + '.csv'\n",
    "    existing_results_folder = 'results_downloader_data/one_minute_data'\n",
    "    \n",
    "    if existing_results_aqy in os.listdir(existing_results_folder):\n",
    "        existing_data_df = pd.read_csv(os.path.join(existing_results_folder, existing_results_aqy))\n",
    "        \n",
    "        return(existing_data_df)\n",
    "    \n",
    "    else:\n",
    "        blank_df = pd.DataFrame()\n",
    "        \n",
    "        return(blank_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts querying the API on the start date generated through the first steps. Subsequently adds seven days onto query start date and end date until reaching yesterday. Appends requested data to existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aqy_data_for_full_deployment(aqc_login, aqy_id):\n",
    "    \n",
    "    date_to_start = check_existing_get_start_date(aqy_id)\n",
    "    \n",
    "    if date_to_start == 'None - up to date': \n",
    "        return(print('Moving on to next AQY - data already up to date'))\n",
    "    else:\n",
    "        query_dates = get_all_query_dates(date_to_start)\n",
    "    \n",
    "    aqy_data_full = initialize_results(aqy_id)\n",
    "    \n",
    "    ### USE THE BELOW TO CRAFT CONDITIONAL EXECUTION FOR SAC DOWNLOAD VS. DEPLOYMENT DOWNLOAD\n",
    "    while query_dates['start_date_dt'] < get_deployment_date(aqy_id):\n",
    "    #while query_dates['start_date_dt'] < datetime.date.today() - datetime.timedelta(days = 1):\n",
    "        print(query_dates['start_date_dt'], query_dates['end_date_dt'])\n",
    "        \n",
    "        request_url = create_request_url(aqy_id, query_dates['start_date_str'], query_dates['end_date_str'])\n",
    "        aqy_data = get_aqy_data_for_week(aqcloud_login, request_url)\n",
    "    \n",
    "        query_dates = get_all_query_dates(query_dates['start_date_dt'] + datetime.timedelta(days = 7))\n",
    "        \n",
    "        if aqy_data.shape[1] > 1:\n",
    "            aqy_data_full = pd.concat([aqy_data_full, aqy_data])\n",
    "    \n",
    "    aqy_data_full['ID'] = aqy_id\n",
    "    aqy_data_full.drop_duplicates(inplace = True)\n",
    "    \n",
    "    print('Data downloaded from cloud and appended to existing file')\n",
    "    \n",
    "    ### GET RID OF _SAC\n",
    "    path_to_new_data = 'results_downloader_data/one_minute_data/sac_raw_minute_{}.csv'.format(aqy_id)\n",
    "    aqy_data_full.to_csv(path_to_new_data, index = False)\n",
    "    \n",
    "    return(aqy_data_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login status: <Response [200]>\n",
      "REQUESTING DATA FOR AQY BB-633\n",
      "2019-12-06 2019-12-12\n",
      "2019-12-13 2019-12-19\n",
      "2019-12-20 2019-12-26\n",
      "2019-12-27 2020-01-02\n",
      "2020-01-03 2020-01-09\n",
      "2020-01-10 2020-01-16\n",
      "2020-01-17 2020-01-23\n",
      "2020-01-24 2020-01-30\n",
      "2020-01-31 2020-02-06\n",
      "2020-02-07 2020-02-13\n",
      "2020-02-14 2020-02-20\n",
      "2020-02-21 2020-02-27\n",
      "2020-02-28 2020-03-05\n",
      "2020-03-06 2020-03-12\n",
      "2020-03-13 2020-03-19\n",
      "2020-03-20 2020-03-26\n",
      "2020-03-27 2020-04-02\n",
      "2020-04-03 2020-04-09\n",
      "2020-04-10 2020-04-16\n",
      "2020-04-17 2020-04-23\n",
      "2020-04-24 2020-04-30\n",
      "2020-05-01 2020-05-07\n",
      "2020-05-08 2020-05-14\n",
      "2020-05-15 2020-05-21\n",
      "2020-05-22 2020-05-28\n",
      "2020-05-29 2020-06-04\n",
      "Data downloaded from cloud and appended to existing file\n",
      "REQUESTING DATA FOR AQY BB-642\n",
      "2019-12-06 2019-12-12\n",
      "2019-12-13 2019-12-19\n",
      "2019-12-20 2019-12-26\n",
      "2019-12-27 2020-01-02\n",
      "2020-01-03 2020-01-09\n",
      "2020-01-10 2020-01-16\n",
      "2020-01-17 2020-01-23\n",
      "2020-01-24 2020-01-30\n",
      "2020-01-31 2020-02-06\n",
      "2020-02-07 2020-02-13\n",
      "2020-02-14 2020-02-20\n",
      "2020-02-21 2020-02-27\n",
      "2020-02-28 2020-03-05\n",
      "2020-03-06 2020-03-12\n",
      "2020-03-13 2020-03-19\n",
      "2020-03-20 2020-03-26\n",
      "2020-03-27 2020-04-02\n",
      "2020-04-03 2020-04-09\n",
      "2020-04-10 2020-04-16\n",
      "2020-04-17 2020-04-23\n",
      "2020-04-24 2020-04-30\n",
      "2020-05-01 2020-05-07\n",
      "2020-05-08 2020-05-14\n",
      "2020-05-15 2020-05-21\n",
      "2020-05-22 2020-05-28\n",
      "2020-05-29 2020-06-04\n",
      "Data downloaded from cloud and appended to existing file\n"
     ]
    }
   ],
   "source": [
    "aqcloud_login = create_aqc_login('lhill@psehealthyenergy.org', 'R1chmond@ir')\n",
    "\n",
    "#aqys_available_list = get_instruments_available(aqcloud_login)\n",
    "#aqys_available_list.remove('AQY BB-896')\n",
    "#aqys_available_list.remove('AQY BB-803')\n",
    "\n",
    "aqys_available_list = ['AQY BB-633', 'AQY BB-642']\n",
    "\n",
    "for aqy in aqys_available_list:\n",
    "    print('REQUESTING DATA FOR', aqy)\n",
    "    get_aqy_data_for_full_deployment(aqcloud_login, aqy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to collect Sacramento Co-location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sac_data():\n",
    "    \n",
    "    aqcloud_login = create_aqc_login('lhill@psehealthyenergy.org', 'R1chmond@ir')\n",
    "\n",
    "    aqys_available_list = get_instruments_available(aqcloud_login)\n",
    "    aqys_available_list.remove('AQY BB-896')\n",
    "\n",
    "    aqys_available_df = pd.DataFrame({'ID': aqys_available_list})\n",
    "    aqys_available_df['start_date'] = aqys_available_df['ID'].apply(check_existing_get_start_date)\n",
    "    first_date_needed = min(set(aqys_available_df['start_date']))\n",
    "    query_dates = get_all_query_dates(first_date_needed)\n",
    "\n",
    "    aqy_data_full = initialize_results('Project')\n",
    "\n",
    "    while query_dates['start_date_dt'] < datetime.date.today() - datetime.timedelta(days = 366):\n",
    "        print(query_dates['start_date_dt'], query_dates['end_date_dt'])\n",
    "\n",
    "        request_url = create_request_url('Project', query_dates['start_date_str'], query_dates['end_date_str'])\n",
    "        aqy_data = get_aqy_data_for_week(aqcloud_login, request_url)\n",
    "\n",
    "        query_dates = get_all_query_dates(query_dates['start_date_dt'] + datetime.timedelta(days = 7))\n",
    "\n",
    "        if aqy_data.shape[1] > 1:\n",
    "            aqy_data_full = pd.concat([aqy_data_full, aqy_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_colocation_minute_data():\n",
    "    aqcloud_login = create_aqc_login('lhill@psehealthyenergy.org', 'R1chmond@ir')\n",
    "\n",
    "    #aqys_available_list = get_instruments_available(aqcloud_login)\n",
    "    #aqys_available_list.remove('AQY BB-896')\n",
    "    \n",
    "    aqys_available_lst = ['AQY BB-633', 'AQY BB-642']\n",
    "    \n",
    "    for aqy in aqys_available_list:\n",
    "\n",
    "        colocation_start = datetime.date(2019, 7, 15)\n",
    "        query_dates = get_all_query_dates(colocation_start)\n",
    "        colocation_end = get_deployment_date(aqy)\n",
    "\n",
    "        aqy_data_full = pd.DataFrame()\n",
    "\n",
    "        while query_dates['start_date_dt'] < colocation_end:\n",
    "\n",
    "            aqy_data = get_aqy_data_for_week(aqcloud_login, aqy, query_dates['start_date_str'], query_dates['end_date_str'])\n",
    "\n",
    "            print('Requested data for', aqy, 'from', query_dates['start_date_dt'], 'to', query_dates['end_date_dt'])\n",
    "\n",
    "            query_dates = get_all_query_dates(query_dates['start_date_dt'] + datetime.timedelta(days = 7))\n",
    "\n",
    "            if aqy_data.shape[1] > 1:\n",
    "                aqy_data_full = pd.concat([aqy_data_full, aqy_data])\n",
    "\n",
    "        aqy_data_full['ID'] = aqy\n",
    "\n",
    "        aqy_data_full.to_csv(os.path.join('results', 'sac_colocation', aqy + '.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login status: <Response [200]>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_aqy_data_for_week() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e44729bfd70a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_colocation_minute_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-34b267dcf579>\u001b[0m in \u001b[0;36mget_colocation_minute_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mquery_dates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_date_dt'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcolocation_end\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0maqy_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_aqy_data_for_week\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maqcloud_login\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maqy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_dates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_date_str'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_dates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_date_str'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Requested data for'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maqy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_dates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_date_dt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'to'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_dates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_date_dt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_aqy_data_for_week() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "get_colocation_minute_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Finds the date to start querying the data based on whether there are missing days throughout the data and whether the most recent timestamp is yesterday. Only called within _check_existing_get_start_date_ if there are existing data. Otherwise, deployment date is used.\n",
    "\n",
    "def find_start_date_from_existing(existing_file_path):\n",
    "    \n",
    "    one_day_difference = datetime.timedelta(days = 1)\n",
    "    \n",
    "    existing_dates = get_existing_dates_in_data(existing_file_path)  \n",
    "    \n",
    "    days_after_gaps = existing_dates[existing_dates['time_dif'] != one_day_difference]\n",
    "    number_of_gaps = days_after_gaps.shape[0]\n",
    "    day_after_last_timestamp = existing_dates['date_observed'][-1:] + one_day_difference\n",
    "    \n",
    "    yesterday = datetime.date.today() - one_day_difference\n",
    "    \n",
    "    if number_of_gaps == 0:\n",
    "        \n",
    "        if day_after_last_timestamp == yesterday:\n",
    "        \n",
    "            print('Go analyze data! This AQY is already up-to-date')\n",
    "\n",
    "            return('Up-to-date')\n",
    "    \n",
    "        else:         \n",
    "            print('No gaps in data, but not quite up to date. Requesting  data starting at:', str(day_after_last_timestamp))\n",
    "        \n",
    "            return(day_after_last_timestamp)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        missing_day = find_first_gap_in_data(days_after_gaps, existing_dates)\n",
    "        print('Data missing starting at ' + str(missing_day) + ': requesting from this date forward')\n",
    "            \n",
    "        return(missing_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creates a list of unique dates in the data. Identifies days that are >1 day after the preceeding day and labels as a gap. Feeds into _find_date_to_start_ if gaps are identified.\n",
    "\n",
    "def look_for_first_gap_in_existing(days_after_gaps, existing_dates):\n",
    "    \n",
    "    day_after_first_gap = days_after_gaps['date_observed'].iloc[0]\n",
    "            \n",
    "    all_data_before_first_gap = existing_dates[existing_dates['date_observed'] < day_after_first_gap]\n",
    "            \n",
    "    day_before_first_gap = all_data_before_first_gap['date_observed'].iloc[-1]\n",
    "        \n",
    "    missing_day = day_before_first_gap + one_day_difference\n",
    "    \n",
    "    return(missing_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_sensors_available(aqc_login, aqy_id):\n",
    "    \n",
    "    aqy_request_url = 'https://cloud.aeroqual.com/api/instrument/{}'.format(aqy_id)\n",
    "    \n",
    "    available_aqy_data = requests.get(aqy_request_url, cookies = aqc_login.cookies)\n",
    "    \n",
    "    if str(available_aqy_data)[-5:-2] != '200':\n",
    "        print('Request for', aqy_id, 'status:', available_aqy_data)\n",
    "        \n",
    "        return(available_aqy_data)\n",
    "    else:\n",
    "        \n",
    "        aqy_metadata = available_aqy_data.json()\n",
    "        \n",
    "        aqy_sensors = [sensor['name'] for sensor in aqy_metadata['sensors']]\n",
    "        \n",
    "        return(aqy_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_data_by_date_network(aqc_login, list_of_aqys, start_date, end_date, avg_time):\n",
    "    \n",
    "    print('Requesting network data for range:', start_date, end_date)\n",
    "    \n",
    "    network_data_df = pd.DataFrame()\n",
    "    \n",
    "    for aqy in list_of_aqys:\n",
    "        \n",
    "        aqy_data_json = get_data_by_aqy(aqc_login, aqy, start_date, end_date, avg_time)\n",
    "\n",
    "        aqy_data_df = pd.json_normalize(aqy_data_json['data'])\n",
    "        aqy_data_df['ID'] = aqy\n",
    "        \n",
    "        if aqy_data_df.shape[1] > 1:\n",
    "            network_data_df = network_data_df.append(aqy_data_df)\n",
    "    \n",
    "    return(network_data_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
